{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline for project 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2000, 2000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tr = pd.read_csv('./data/train.csv')\n",
    "va = pd.read_csv('./data/valid.csv')\n",
    "ts = pd.read_csv('./data/test.csv')\n",
    "len(tr), len(va), len(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    20000\n",
       "test      2000\n",
       "valid     2000\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add flag to identify where data is from\n",
    "tr['category'] = 'train'\n",
    "va['category'] = 'valid'\n",
    "ts['category'] = 'test'\n",
    "df = pd.concat([tr, va, ts])\n",
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'cool', 'date', 'funny', 'review_id', 'stars', 'text',\n",
       "       'useful', 'user_id', 'category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polarity and subjectivity from TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'cool', 'date', 'funny', 'review_id', 'stars', 'text',\n",
       "       'useful', 'user_id', 'category', 'subjectivity', 'polarity'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "sub = []\n",
    "pol = []\n",
    "for txt in df['text']:\n",
    "    blob = TextBlob(txt).sentiment\n",
    "    sub.append(blob.subjectivity)\n",
    "    pol.append(blob.polarity)\n",
    "\n",
    "df['subjectivity'] = sub\n",
    "df['polarity'] = pol\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postive, negative and objective score from SentiWorldNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.01785714 0.         0.98214286]\n",
      "[None None None]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "def get_senti_word(s):\n",
    "    lst = list(swn.senti_synsets(s))\n",
    "    if len(lst) == 0:\n",
    "        return np.array([None] * 3)\n",
    "    else:\n",
    "        pos = np.array([i.pos_score() for i in lst])\n",
    "        neg = np.array([i.neg_score() for i in lst])\n",
    "        obj = np.array([i.obj_score() for i in lst])\n",
    "        return np.array([pos.mean(), neg.mean(), obj.mean()])\n",
    "\n",
    "print(get_senti_word('life'))\n",
    "print(get_senti_word('safsdfsdg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.1875, 0.1875, 0.1875, 0.1875, 0.175, 0.175, 0.175, 0.175, 0.6375, 0.6375, 0.6375, 0.6375)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def get_senti_txt(txt):\n",
    "    try:\n",
    "        txt = nltk.word_tokenize(txt)\n",
    "#         ps = PorterStemmer()\n",
    "#         txt = [ps.stem(token) for token in txt]\n",
    "        txt = [re.sub(r'\\W+', '', s) for s in txt]\n",
    "        txt = [token for token in txt if token not in stopwords and not token.isnumeric()]\n",
    "\n",
    "        txt_score = [get_senti_word(s) for s in txt]\n",
    "        txt_pos = list(filter(None.__ne__, [s[0] for s in txt_score]))\n",
    "        txt_neg = list(filter(None.__ne__, [s[1] for s in txt_score]))\n",
    "        txt_obj = list(filter(None.__ne__, [s[2] for s in txt_score]))\n",
    "\n",
    "        return np.mean(txt_pos), np.median(txt_pos), np.max(txt_pos), np.min(txt_pos), \\\n",
    "                np.mean(txt_neg), np.median(txt_neg), np.max(txt_neg), np.min(txt_neg), \\\n",
    "                np.mean(txt_obj), np.median(txt_obj), np.max(txt_obj), np.min(txt_obj)\n",
    "    except:\n",
    "        return np.array([None] * 12)\n",
    "\n",
    "print(get_senti_txt('This is exciting!!!'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'cool', 'date', 'funny', 'review_id', 'stars', 'text',\n",
       "       'useful', 'user_id', 'category', 'subjectivity', 'polarity', 'pos_mean',\n",
       "       'pos_median', 'pos_high', 'pos_low', 'neg_mean', 'neg_median',\n",
       "       'neg_high', 'neg_low', 'obj_mean', 'obj_median', 'obj_high', 'obj_low'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pos_mean'], df['pos_median'], df['pos_high'], df['pos_low'], \\\n",
    "df['neg_mean'], df['neg_median'], df['neg_high'], df['neg_low'], \\\n",
    "df['obj_mean'], df['obj_median'], df['obj_high'], df['obj_low'], \\\n",
    "= zip(*df['text'].map(get_senti_txt))\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the data, as run all above would take much time\n",
    "# df.to_csv('./data_basic.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cool', 'funny', 'useful', 'subjectivity', 'polarity', 'pos_mean', 'pos_median', 'pos_high', 'pos_low', 'neg_mean', 'neg_median', 'neg_high', 'neg_low', 'obj_mean', 'obj_median', 'obj_high', 'obj_low']\n"
     ]
    }
   ],
   "source": [
    "# Gather feature together\n",
    "features = ['cool', 'funny', 'useful'] + list(df.columns[10:])\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 20000, 2000, 2000, 2000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.fillna(0)\n",
    "\n",
    "X = df[df.category == 'train'][features].astype(float)\n",
    "y = df[df.category == 'train']['stars']\n",
    "X_val = df[df.category == 'valid'][features].astype(float)\n",
    "y_val = df[df.category == 'valid']['stars']\n",
    "X_test = df[df.category == 'test'][features].astype(float)\n",
    "\n",
    "len(X), len(y), len(X_val), len(y_val), len(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# from tqdm import tqdm\n",
    "# from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# train = lgb.Dataset(data=X, label=y)\n",
    "\n",
    "# # tune the parameter with grid search (with cross validation)\n",
    "# param_grid = {\n",
    "#     'max_depth': [5, 7, None],\n",
    "#     'min_data_in_leaf': [1, 4, 8, 16],\n",
    "#     'feature_fraction': [0.3, 0.5, 0.7]\n",
    "# }\n",
    "\n",
    "# cv_results = []\n",
    "\n",
    "# for hyperparams in tqdm(list(ParameterGrid(param_grid))):\n",
    "#     fixed = {\n",
    "#         \"objective\": 'regression',\n",
    "#         \"learning_rate\": 0.01\n",
    "#     }\n",
    "#     hyperparams.update(fixed)\n",
    "\n",
    "#     validation_summary = lgb.cv(\n",
    "#         hyperparams,\n",
    "#         train,\n",
    "#         num_boost_round=6000,\n",
    "#         nfold=5,\n",
    "#         metrics=[\"rmse\"],\n",
    "#         early_stopping_rounds=500,\n",
    "#         verbose_eval=None)\n",
    "\n",
    "#     optimal_num_trees = len(validation_summary[\"rmse-mean\"])\n",
    "#     hyperparams[\"num_boost_round\"] = optimal_num_trees\n",
    "#     cv_results.append((hyperparams, validation_summary[\"rmse-mean\"][-1]))\n",
    "\n",
    "# # save the tuning results\n",
    "# tmp = pd.DataFrame(cv_results)\n",
    "# tmp.to_csv('./lgb_cv_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import lightgbm as lgb\n",
    "tmp = pd.read_csv('./lgb_cv_result.csv')\n",
    "param = ast.literal_eval(list(tmp.sort_values(by=['1'])['0'])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract best parameter set with least rmse\n",
    "# param = list(tmp.sort_values(by=[1])[0])[0]\n",
    "train = lgb.Dataset(data=X, label=y)\n",
    "gbm = lgb.train(\n",
    "    param, \n",
    "    train_set=train, \n",
    "    num_boost_round=param['num_boost_round'])\n",
    "\n",
    "# save feature importance to csv\n",
    "# importance = pd.DataFrame([X.columns, gbm.feature_importance()]).T\n",
    "# importance.columns = ['feature', 'value']\n",
    "# importance = importance.sort_values(by=['value'], ascending=False)\n",
    "# importance.to_csv('./lgb_importance.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if predicted stars are out of bound, cast to 1 or 5; round to nearest integer otherwise\n",
    "def round_star(star):\n",
    "    if star < 1:\n",
    "        return 1\n",
    "    elif star > 5:\n",
    "        return 5\n",
    "    else:\n",
    "        return round(star)\n",
    "\n",
    "pre = df[df.category == 'test'][['review_id']]\n",
    "pre['predicted_star'] = [round_star(s) for s in gbm.predict(X_test)]\n",
    "pre.to_csv('./lgb_predict.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM: \n",
      "accuracy: 0.4485 \tprecision: 0.4687899120153495 \trecall: 0.40073731519981315 \tf1: 0.39445434093244286\n"
     ]
    }
   ],
   "source": [
    "# LightGBM\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "y_val_pre = [round_star(s) for s in gbm.predict(X_val)]\n",
    "acc = accuracy_score(y_val, y_val_pre)\n",
    "p, r, f1, _ = precision_recall_fscore_support(y_val, y_val_pre, average=\"macro\")\n",
    "print('LightGBM: ')\n",
    "print(\"accuracy:\", acc, \"\\tprecision:\", p, \"\\trecall:\", r, \"\\tf1:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    20000\n",
       "test      2000\n",
       "valid     2000\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = df[df.category == 'test'][['review_id']]\n",
    "pre['predicted_star'] = gbm.predict(X_test)\n",
    "pre.to_csv('./data/lgb_pred.csv', index=False)\n",
    "\n",
    "val = df[df.category == 'valid'][['review_id']]\n",
    "val['predicted_star'] = gbm.predict(X_val)\n",
    "val.to_csv('./data/lgb_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
